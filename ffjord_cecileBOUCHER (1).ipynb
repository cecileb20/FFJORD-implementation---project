{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6LLC6x84sHv"
      },
      "outputs": [],
      "source": [
        "### FFJORD_Implementation.ipynb\n",
        "# By Cecile Marine BOUCHER, INTRO TO AI class, 2025/10/26\n",
        "# This code implements FFJORD model based on the paper by Will Grathwohl,\n",
        "# Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, David Duvenaud\n",
        "# (paper link : arXiv:1810.01367)\n",
        "\n",
        "# Install the required library for ODEs (Ordinary Differential Equations)\n",
        "!pip install torchdiffeq\n",
        "\n",
        "# Standard imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "# The main tool for this project\n",
        "from torchdiffeq import odeint_adjoint as odeint\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(42)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Parameters\n",
        "\n",
        "# MNIST has 28x28 images\n",
        "data_dim = 28 * 28\n",
        "SUBSET_PERCENTAGE = 0.4\n",
        "BATCH_SIZE = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for the data\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # Add uniform noise for dequantization (standard practice for continuous flows)\n",
        "    transforms.Lambda(lambda x: x + torch.rand_like(x) / 256.),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    # Flatten the 28x28 image into a 784-dimensional vector\n",
        "    transforms.Lambda(lambda x: x.view(-1)),])\n",
        "\n",
        "# Download the dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# SUBSET SELECTION (40%)\n",
        "num_samples = len(train_dataset)\n",
        "subset_size = int(SUBSET_PERCENTAGE * num_samples)\n",
        "indices = torch.randperm(num_samples)[:subset_size]\n",
        "train_subset = torch.utils.data.Subset(train_dataset, indices)\n",
        "\n",
        "print(f\"Using {subset_size} samples ({SUBSET_PERCENTAGE*100}%) of the MNIST dataset.\")\n",
        "\n",
        "train_loader = DataLoader(dataset=train_subset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "SpUbb0dX5wua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### FFJORD Model Definition\n",
        "\n",
        "'''\n",
        "The neural network for the dynamics f(z(t), t)\n",
        "\n",
        "This network will learn the flow dynamics. It takes as input the current position z(t) and time 't' and returns the \"velocity\" dz/dt.\n",
        "This is the core of the model, parameterized by 'θ'.\n",
        "'''\n",
        "\n",
        "#### Dynamics Network Code\n",
        "\n",
        "class DynamicsNet(nn.Module):\n",
        "    def __init__(self, hidden_dim=256):\n",
        "        super(DynamicsNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input dimension is data_dim (784) + 1 (for time t)\n",
        "            nn.Linear(data_dim + 1, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            # Output dimension is data_dim (784)\n",
        "            nn.Linear(hidden_dim, data_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, t, z):\n",
        "        # We append time t as an extra dimension to each element of the batch\n",
        "        if t.dim() == 0: t = t.expand(z.size(0))\n",
        "        t = t.view(-1, 1)\n",
        "        # Concatenate the state z and the time t before passing them to the network\n",
        "        return self.net(torch.cat([z, t], dim=1))"
      ],
      "metadata": {
        "id": "iYT4UlUc52uQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### The Hutchinson Estimator\n",
        "\n",
        "def hutchinson_trace_estimator(dynamics_fn, z, t, n_epsilons=1):\n",
        "    \"\"\"\n",
        "    Estimates the trace of the Jacobian of dynamics_fn with respect to z.\n",
        "    \"\"\"\n",
        "\n",
        "    with torch.enable_grad():\n",
        "      z.requires_grad_(True)\n",
        "\n",
        "      trace_estimates = []\n",
        "      for _ in range(n_epsilons):\n",
        "          # Sample a noise vector epsilon.\n",
        "          # The paper mentions a Rademacher or Gaussian distribution.\n",
        "          # We use Rademacher (values -1 or 1).\n",
        "          epsilon = torch.randint(low=0, high=2, size=z.shape, device=z.device) * 2 - 1\n",
        "          epsilon = epsilon.float()\n",
        "\n",
        "          # Calculate the product f(z, t) * epsilon.\n",
        "          # This is the first part of the \"vector-Jacobian product\" (VJP).\n",
        "          f_val = dynamics_fn(t, z)\n",
        "\n",
        "          # Calculate the vector-Jacobian product (VJP).\n",
        "          # This is the autodiff step : we compute (epsilon^T * J).\n",
        "          # torch.autograd.grad calculates the gradient of (f_val * epsilon).sum() with respect to z.\n",
        "          vjp_result, = torch.autograd.grad(f_val, z, grad_outputs=epsilon, create_graph=True, retain_graph=True)\n",
        "\n",
        "          # Calculate the final product (VJP * epsilon) to get the trace estimate.\n",
        "          trace_estimate = torch.sum(vjp_result * epsilon, dim=1)\n",
        "          trace_estimates.append(trace_estimate)\n",
        "\n",
        "      # Average the estimates if n_epsilons > 1.\n",
        "      final_trace = torch.mean(torch.stack(trace_estimates), dim=0)\n",
        "\n",
        "    return final_trace"
      ],
      "metadata": {
        "id": "G8mBgMeT591a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "### FFJORD Model Assembly\n",
        "\n",
        "We combine the two parts. The model's forward method will solve an ODE that combines the transformation of 'z' and\n",
        "the calculation of the log-density change.\n",
        "'''\n",
        "\n",
        "#### --------------FFJORD MODEL-------------------------\n",
        "\n",
        "class AugmentedDynamics(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the augmented derivative vector f_aug(t, z_aug) = [dz/dt, d(log P)/dt, d(Kinetic Reg)/dt].\n",
        "    This is the function that torchdiffeq will call repeatedly (at each step).\n",
        "    \"\"\"\n",
        "    def __init__(self, dynamics_net, n_epsilons):\n",
        "        super().__init__()\n",
        "        self.dynamics_net = dynamics_net\n",
        "        self.n_epsilons = n_epsilons\n",
        "        self.data_dim = 784\n",
        "\n",
        "    def forward(self, t, state):\n",
        "        # z is the first data_dim components. The state is  [z, log_p, kinetic_reg]\n",
        "        z = state[:, :self.data_dim]\n",
        "\n",
        "        # dz/dt = f(z, t)\n",
        "        dz_dt = self.dynamics_net(t, z)\n",
        "\n",
        "        # d(log p)/dt = -Tr(∂f/∂z)\n",
        "        dlogp_dt = -hutchinson_trace_estimator(self.dynamics_net, z, t, self.n_epsilons)\n",
        "\n",
        "        # d(Kinetic Reg)/dt = 0.5 * ||f(z, t)||^2\n",
        "        dkinetic_dt = 0.5 * torch.sum(dz_dt ** 2, dim=1)\n",
        "\n",
        "        # The augmented state derivatives are concatenated in the order: [dz/dt, d(log p)/dt, d(Kinetic Reg)/dt]\n",
        "        return torch.cat([dz_dt,\n",
        "                          dlogp_dt.view(-1, 1),\n",
        "                          dkinetic_dt.view(-1, 1)], dim=1)\n",
        "\n",
        "class FFJORD(nn.Module):\n",
        "    def __init__(self, dynamics_net, n_epsilons=1, lambda_reg=0.01):\n",
        "        super().__init__()\n",
        "        self.dynamics_net = dynamics_net\n",
        "        self.n_epsilons = n_epsilons\n",
        "        self.lambda_reg = lambda_reg\n",
        "        # We integrate from t=1 to t=0 for inference (used during training and evaluation/loss calculation)\n",
        "        self.integration_times_inference = torch.tensor([1.0, 0.0]).to(device)\n",
        "        # We integrate from t=0 to t=1 for generation (model creates new samples after training)\n",
        "        self.integration_times_generation = torch.tensor([0.0, 1.0]).to(device)\n",
        "\n",
        "        self.augmented_dynamics = AugmentedDynamics(dynamics_net, n_epsilons)\n",
        "\n",
        "        # SOLVER PARAMETERS\n",
        "        self.method = 'rk4'\n",
        "        self.rtol = 1e-7 # Relative tolerance\n",
        "        self.atol = 1e-8 # Absolute tolerance\n",
        "        self.solver_options = dict(step_size=0.03)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Base distribution, a Gaussian\n",
        "        base_dist = torch.distributions.MultivariateNormal(\n",
        "            loc=torch.zeros(data_dim).to(device),\n",
        "            covariance_matrix=torch.eye(data_dim).to(device)\n",
        "        )\n",
        "\n",
        "        # Initial log-probability (we start with a delta of 0).\n",
        "        log_px = torch.zeros(x.shape[0], 1).to(device)\n",
        "\n",
        "        # Initialize the kinetic regularization integral to 0\n",
        "        kinetic_reg = torch.zeros(x.shape[0], 1).to(device)\n",
        "\n",
        "        # We will solve the ODE for [z(t), log_p(z(t)), kinetic_reg] simultaneously.\n",
        "        # We therefore concatenate x, the initial log_px, and kinetic_reg.\n",
        "        augmented_state = torch.cat([x, log_px, kinetic_reg], dim=1)\n",
        "\n",
        "        # ODE Solution\n",
        "        # We integrate from t=1 to t=0\n",
        "        solution = odeint(\n",
        "            self.augmented_dynamics,\n",
        "            augmented_state,\n",
        "            self.integration_times_inference,\n",
        "            method=self.method,\n",
        "            rtol=self.rtol,\n",
        "            atol=self.atol,\n",
        "            options=self.solver_options\n",
        "        )\n",
        "\n",
        "        # We retrieve z(t0), the total log-probability change, and the regularization term\n",
        "        z_t0 = solution[1][:, :data_dim]\n",
        "        # delta_log_p is the 2nd state (index data_dim)\n",
        "        delta_log_p = solution[1][:, data_dim:data_dim+1]\n",
        "        # delta_kinetic_reg is the 3rd state (index data_dim + 1)\n",
        "        delta_kinetic_reg = solution[1][:, data_dim+1:]\n",
        "\n",
        "        # Final log-probability: log p(z0) - ∫ Tr(df/dz) dt\n",
        "        log_pz0 = base_dist.log_prob(z_t0)\n",
        "        final_log_px = log_pz0.view(-1, 1) - delta_log_p\n",
        "\n",
        "        # Kinetic regularization loss: lambda_reg * ∫ 0.5 * ||f||^2 dt\n",
        "        reg_loss_term = self.lambda_reg * delta_kinetic_reg\n",
        "\n",
        "        return final_log_px, reg_loss_term\n",
        "\n",
        "    def generate(self, n_samples=64):\n",
        "        # We start from the base distribution\n",
        "        z0 = torch.rand(n_samples, data_dim) * 2 - 1\n",
        "        z0 = z0.to(device)\n",
        "\n",
        "        # Solve the ODE from t=0 to t=1 for generation\n",
        "        solution = odeint(\n",
        "            dynamics_net,\n",
        "            z0,\n",
        "            self.integration_times_generation,\n",
        "            method=self.method,\n",
        "            rtol=self.rtol,\n",
        "            atol=self.atol,\n",
        "            options=self.solver_options\n",
        "        )\n",
        "\n",
        "        # Get the final state z(t=1)\n",
        "        z_t1 = solution[-1]\n",
        "\n",
        "        # Force the generation to be between -1 and 1\n",
        "        z_t1 = torch.tanh(z_t1)\n",
        "        return z_t1"
      ],
      "metadata": {
        "id": "Usf-xGmv6AlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Image Display for Generation\n",
        "\n",
        "def plot_generations(images, title=\"Generated images\"):\n",
        "    fig, axes = plt.subplots(2, 8, figsize=(10,4))\n",
        "    axes = axes.flatten()\n",
        "    for i in range(min(16, len(images))):\n",
        "        img = images[i].view(28, 28).cpu().numpy()\n",
        "        # Normalize to [0, 1]\n",
        "        img_norm = (img - img.min()) / (img.max() - img.min())\n",
        "        # If the background is too light, invert the image\n",
        "        if img_norm.mean() > 0.5:\n",
        "            img_norm = 1 - img_norm\n",
        "        axes[i].imshow(img_norm, cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    fig.suptitle(title) # Set the title for the entire figure\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "uTUHrmYf6DaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### --------------TRAINING-----------------------------\n",
        "\n",
        "# Parameters\n",
        "n_epochs = 100\n",
        "learning_rate = 1e-3\n",
        "n_epsilons_training = 1\n",
        "lambda_reg = 0.01\n",
        "\n",
        "# Initialization\n",
        "dynamics_net = DynamicsNet().to(device)\n",
        "ffjord_model = FFJORD(dynamics_net, n_epsilons=n_epsilons_training, lambda_reg=lambda_reg).to(device)\n",
        "optimizer = torch.optim.Adam(ffjord_model.parameters(), lr=learning_rate, weight_decay = 1e-5)\n",
        "\n",
        "# Initialize scheduler\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "print(\"Beginning of training...\")\n",
        "start_time = time.time()\n",
        "for epoch in tqdm.tqdm(range(n_epochs+1)):\n",
        "    total_loss = 0.0\n",
        "    ffjord_model.train()\n",
        "\n",
        "    for i, (x_batch, _) in enumerate(train_loader):\n",
        "\n",
        "        x_batch = x_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # ffjord_model returns log-probability and regularization term\n",
        "        log_prob, reg_loss_term = ffjord_model(x_batch)\n",
        "\n",
        "        # Negative Log-Likelihood (NLL) Loss\n",
        "        nll_loss = -torch.mean(log_prob)\n",
        "\n",
        "        # Total Loss = NLL Loss + Kinetic Regularization Loss\n",
        "        loss = nll_loss + torch.mean(reg_loss_term)\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(ffjord_model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{n_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    scheduler.step(avg_train_loss)\n",
        "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Generation\n",
        "    ffjord_model.eval()\n",
        "    with torch.no_grad():\n",
        "        generated_images = ffjord_model.generate(n_samples=16)\n",
        "    # Display\n",
        "    if generated_images is not None:\n",
        "        # Example of generation after training\n",
        "        plot_generations(generated_images, title=f\"Generation (K={n_epsilons_training})\")\n",
        "\n",
        "\n",
        "    print(f'--- Epoch {epoch+1} finished, Average Loss: {total_loss / len(train_loader):.4f} ---')\n",
        "end_time = time.time()-start_time\n",
        "print(\"Training finished. process time: {}sec\".format(end_time))"
      ],
      "metadata": {
        "id": "k3Irg58e6J28"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}